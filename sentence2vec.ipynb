{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from scipy.misc import imread, imresize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import vist\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(sentences_ids):\n",
    "    \n",
    "    sentences = {}\n",
    "    \n",
    "    for sent_id in sentences_ids:\n",
    "        sentences[sent_id] = sentences_ids[sent_id]['text']\n",
    "        \n",
    "    return sentences\n",
    "    \n",
    "    \n",
    "    \n",
    "def multiple_terms_to_vec(caption, model):\n",
    "    \"\"\"\"\n",
    "    Builds a uniform representation for the terms of a caption\n",
    "    by summing the corresponding word vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # split caption in terms\n",
    "    terms = caption.split()\n",
    "    \n",
    "    # sum of the vectors of each term\n",
    "    sum_of_vecs = np.zeros(300)\n",
    "\n",
    "    # counter for words that have wordvector\n",
    "    length = 0\n",
    "    \n",
    "    # sum the corresponding word vectors that are available \n",
    "    for term in terms:\n",
    "        if term in model and term not in stopwords:\n",
    "            sum_of_vecs += model[term]\n",
    "            length += 1\n",
    "    \n",
    "    if length == 0:\n",
    "        length = 1\n",
    "\n",
    "    return (sum_of_vecs/length)\n",
    "\n",
    "\n",
    "\n",
    "def store_sentences2vec(path, sentences, w2v_model):\n",
    "\n",
    "    with open(path, \"w\") as text_file:\n",
    "            text_file.write(str(len(sentences))+\" 300\\n\")\n",
    "\n",
    "    for key, value in sentences.items():\n",
    "\n",
    "        vec_c = multiple_terms_to_vec(value, w2v_model)\n",
    "\n",
    "        with open(path, \"a\") as text_file:\n",
    "            text_file.write(key+\" \")\n",
    "\n",
    "            for i in vec_c:\n",
    "                text_file.write(str(i)+\" \")\n",
    "            text_file.write(\"\\n\")\n",
    "        #print (key,vec_c,value)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "def demo_visualize_sentences2imgs(path, sentence_1, sentence_2, sentences_id):\n",
    "    \n",
    "    my_path = path\n",
    "\n",
    "    sentence_1_id = sentence_1\n",
    "    sentence_2_id = sentence_2\n",
    "\n",
    "    filename = my_path+sis.Sents[sentence_1_id]['img_id']+\".jpg\"\n",
    "    img = imread(filename, mode='RGB')\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(filename+\" - \"+sis.Sents[sentence_1_id]['text'])\n",
    "    plt.show()\n",
    "\n",
    "    filename = my_path+sis.Sents[sentence_2_id]['img_id']+\".jpg\"\n",
    "    img = imread(filename, mode='RGB')\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(filename+\" - \"+sis.Sents[sentence_2_id]['text'])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the sentences and create the sentence2vec representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Getting the sentences id...............\")\n",
    "\n",
    "vist_images_dir = 'data\\images'\n",
    "vist_annotations_dir = 'data'\n",
    "dataset_type = 'val'\n",
    "\n",
    "sis = vist.Story_in_Sequence(vist_images_dir, vist_annotations_dir, [dataset_type])\n",
    "\n",
    "\n",
    "\n",
    "print (\"Getting the sentences...............\")\n",
    "\n",
    "sentences = get_sentences(sis.Sents)\n",
    "\n",
    "\n",
    "print (\"Creating the sentences2vec representations and store them in file...............\")\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "path = dataset_type+\"_sentences.txt\"\n",
    "store_sentences2vec(path, sentences, w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load sentences2vec model\n",
    "\n",
    "print (\"Loading the sentences2vec model...............\")\n",
    "\n",
    "dataset_type = 'val'\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(dataset_type+\"_sentences.txt\", binary=False)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load sentences from Vist\n",
    "\n",
    "print (\"Getting the sentences id...............\")\n",
    "\n",
    "vist_images_dir = 'data\\images'\n",
    "vist_annotations_dir = 'data'\n",
    "dataset_type = 'val'\n",
    "\n",
    "sis = vist.Story_in_Sequence(vist_images_dir, vist_annotations_dir, [dataset_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the 10th most similar sentences given the first sentence in the Vist dataset\n",
    "\n",
    "most_similar = w2v_model.most_similar(list(sis.Sents)[0])\n",
    "\n",
    "\n",
    "# print the input sentence and the most similar one \n",
    "\n",
    "print (\"The input sentence with sentence id \"+list(sis.Sents)[0]+\n",
    "       \" is the following : \\n\\n\"+ sis.Sents[list(sis.Sents)[0]]['text'])\n",
    "\n",
    "print (\"\\n\\nThe most similar sentence is the sentence with sentence id \"\n",
    "       +most_similar[0][0]+\" :\\n\\n\"+ sis.Sents[most_similar[0][0]]['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize the images that belong to two sentences\n",
    "\n",
    "path ='data/images/val/'\n",
    "\n",
    "sentence_1_id = list(sis.Sents)[0]\n",
    "sentence_2_id = most_similar[0][0]\n",
    "\n",
    "demo_visualize_sentences2imgs(path, sentence_1_id, sentence_2_id, sis.Sents)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
