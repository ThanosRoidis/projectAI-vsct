{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensoflow MLP source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA Multilayer Perceptron implementation example using TensorFlow library.\\nThis example is using the MNIST database of handwritten digits\\n(http://yann.lecun.com/exdb/mnist/)\\n\\nAuthor: Aymeric Damien\\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "A Multilayer Perceptron implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    " \n",
    "import vist\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sis's [val] loaded. It took 0.42 seconds.\n",
      "Make mapping ...\n",
      "Mapping for [Albums][Images][Stories][Sents] done.\n",
      "Topics (#albums) (#stories)\n",
      "0.4th_of_july (44)(220)\n",
      "1.MISC (60)(300)\n",
      "2.amusement_park (103)(515)\n",
      "3.award_ceremony (16)(80)\n",
      "4.baby_shower (5)(25)\n",
      "5.bar_mitzvah (8)(40)\n",
      "6.bat_mitzvah (2)(10)\n",
      "7.beach (150)(750)\n",
      "8.bike_race (29)(145)\n",
      "9.birthday (87)(435)\n",
      "10.birthday_party (20)(100)\n",
      "11.breaking_up (77)(385)\n",
      "12.building_a_house (98)(490)\n",
      "13.camping (1)(5)\n",
      "14.car_accident (26)(130)\n",
      "15.carnival (57)(285)\n",
      "16.christmas (20)(100)\n",
      "17.church (47)(235)\n",
      "18.cinco_de_mayo (20)(100)\n",
      "19.cooking_dinner (19)(95)\n",
      "20.day_parade (23)(115)\n",
      "21.easter (50)(250)\n",
      "22.eid_al-adha (2)(10)\n",
      "23.eid_al-fitr (3)(15)\n",
      "24.engagement (17)(85)\n",
      "25.fair (14)(70)\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/img2features\", 'rb') as file:\n",
    "        img2features = pickle.load(file)\n",
    "        \n",
    "vist_images_dir = 'data/val.images'\n",
    "vist_annotations_dir = 'data'\n",
    "sis = vist.Story_in_Sequence(vist_images_dir, vist_annotations_dir, ['val'])\n",
    "\n",
    "\n",
    "topic2albums = {} \n",
    "\n",
    "for album in sis.Albums.values():\n",
    "    album_id = album['id']\n",
    "    vist_label = album[\"vist_label\"]\n",
    "    \n",
    "    if not vist_label in topic2albums: \n",
    "        topic2albums[vist_label] = [album_id]\n",
    "    else: \n",
    "        topic2albums[vist_label].append(album_id)\n",
    "        \n",
    "        \n",
    "topic2stories = {}\n",
    "\n",
    "for topic in topic2albums:\n",
    "    topic2stories[topic] = []\n",
    "    \n",
    "    topic_albums = topic2albums[topic];\n",
    "    for album_id in topic_albums:\n",
    "        stories = sis.Albums[album_id]['story_ids']\n",
    "        topic2stories[topic].extend(stories)\n",
    "        \n",
    "    \n",
    "\n",
    "print(\"Topics (#albums) (#stories)\")\n",
    "for i,key in enumerate(sorted(topic2albums.keys())):         \n",
    "    print(\"%d.%s (%d)(%d)\"%(i,key, len(topic2albums[key]), len(topic2stories[key])))\n",
    "\n",
    "\n",
    "sis.Topics = Topics = {topic : {'story_ids' : topic2stories[topic], 'album_ids' : topic2albums[topic]} for topic in topic2stories}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1408\n"
     ]
    }
   ],
   "source": [
    "def convert_one_hot(labels_list):\n",
    "    one_hot = [0] * 5\n",
    "#     for order in labels_list:\n",
    "#         one_hot[order] = 1\n",
    "    data = Counter(labels_list)\n",
    "    one_hot[data.most_common(1)[0][0]] = 1\n",
    "    return one_hot\n",
    "\n",
    "#get images that belong in a story\n",
    "img2labels = {}\n",
    "for im_name in img2features:\n",
    "    img_id = im_name.split('.')[0]\n",
    "    img2labels[img_id] = []\n",
    "    \n",
    "for story in sis.Stories.values():\n",
    "    for order, img_id in enumerate(story['img_ids']):\n",
    "        img2labels[img_id].append(order)\n",
    "    \n",
    "# shuffle data\n",
    "# r_img_id = random.choice(list(img2labels.keys()))\n",
    "# print(r_img_id)\n",
    "# print(img2labels[r_img_id])\n",
    "\n",
    "img2labels = {img_id : convert_one_hot(img2labels[img_id]) for img_id in img2labels if img2labels[img_id]}\n",
    "\n",
    "\n",
    "#keep images that belong in relevant topics\n",
    "relevant_topics = [\"4th_of_july\", \"bike_race\", \"award_ceremony\", \"birthday\"]\n",
    "relevant_img_ids = set([])\n",
    "\n",
    "for topic in relevant_topics:\n",
    "    for story_id in sis.Topics[topic][\"story_ids\"]:\n",
    "        relevant_img_ids.update(sis.Stories[story_id][\"img_ids\"])\n",
    "\n",
    "img2labels = {img_id : img2labels[img_id] for img_id in img2labels if img_id in relevant_img_ids}\n",
    "        \n",
    "        \n",
    "print(\"Images that belong in relevant topics: %d\", %(len(relevant_img_ids)))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test split: 80% / 19 %\n",
      "Train size: 1126\n",
      "Test size: 282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19999999999999996"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.zeros((len(img2labels), 4096))\n",
    "Y = np.zeros((len(img2labels), 5))\n",
    "\n",
    "for i, img_id in enumerate(img2labels):\n",
    "\n",
    "    X[i,:] = img2features[img_id + '.jpg']\n",
    "    Y[i,:] = img2labels[img_id]\n",
    "    \n",
    "train_per = 0.8\n",
    "train_features = X[:int(0.8*X.shape[0]), :]\n",
    "train_labels = Y[:int(0.8*X.shape[0]), :]\n",
    "\n",
    "test_features = X[int(0.8*X.shape[0]):, :]\n",
    "test_labels = Y[int(0.8*X.shape[0]):, :]\n",
    "\n",
    "print(\"Train/test split: %d%% / %d %%\" % (train_per * 100,(1-train_per) * 100))\n",
    "print(\"Train size: %d\" %(len(train_features)))\n",
    "print(\"Test size: %d\" %(len(test_features)))\n",
    "\n",
    "(1-train_per)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 16225.125710227\n",
      " Train accuracy: 0.214032\n",
      " Test accuracy: 0.202128\n",
      "Epoch: 0002 cost= 13396.907049006\n",
      " Train accuracy: 0.23357\n",
      " Test accuracy: 0.180851\n",
      "Epoch: 0003 cost= 12050.147460938\n",
      " Train accuracy: 0.262877\n",
      " Test accuracy: 0.184397\n",
      "Epoch: 0004 cost= 10702.319602273\n",
      " Train accuracy: 0.280639\n",
      " Test accuracy: 0.212766\n",
      "Epoch: 0005 cost= 9729.784446023\n",
      " Train accuracy: 0.293961\n",
      " Test accuracy: 0.216312\n",
      "Epoch: 0006 cost= 9271.381081321\n",
      " Train accuracy: 0.324156\n",
      " Test accuracy: 0.216312\n",
      "Epoch: 0007 cost= 8561.419611151\n",
      " Train accuracy: 0.343695\n",
      " Test accuracy: 0.22695\n",
      "Epoch: 0008 cost= 7786.229980469\n",
      " Train accuracy: 0.360568\n",
      " Test accuracy: 0.22695\n",
      "Epoch: 0009 cost= 7843.852627841\n",
      " Train accuracy: 0.376554\n",
      " Test accuracy: 0.205674\n",
      "Epoch: 0010 cost= 6719.494273793\n",
      " Train accuracy: 0.402309\n",
      " Test accuracy: 0.191489\n",
      "Epoch: 0011 cost= 6497.236550071\n",
      " Train accuracy: 0.437833\n",
      " Test accuracy: 0.212766\n",
      "Epoch: 0012 cost= 6546.509499290\n",
      " Train accuracy: 0.455595\n",
      " Test accuracy: 0.205674\n",
      "Epoch: 0013 cost= 5895.666681463\n",
      " Train accuracy: 0.48135\n",
      " Test accuracy: 0.212766\n",
      "Epoch: 0014 cost= 5472.881835938\n",
      " Train accuracy: 0.487567\n",
      " Test accuracy: 0.212766\n",
      "Epoch: 0015 cost= 5707.845081676\n",
      " Train accuracy: 0.508881\n",
      " Test accuracy: 0.223404\n",
      "Epoch: 0016 cost= 5011.768821023\n",
      " Train accuracy: 0.538188\n",
      " Test accuracy: 0.223404\n",
      "Epoch: 0017 cost= 4479.360218395\n",
      " Train accuracy: 0.563055\n",
      " Test accuracy: 0.219858\n",
      "Epoch: 0018 cost= 4590.941250888\n",
      " Train accuracy: 0.579041\n",
      " Test accuracy: 0.219858\n",
      "Epoch: 0019 cost= 3902.485040838\n",
      " Train accuracy: 0.592362\n",
      " Test accuracy: 0.216312\n",
      "Epoch: 0020 cost= 3693.727139560\n",
      " Train accuracy: 0.605684\n",
      " Test accuracy: 0.205674\n",
      "Epoch: 0021 cost= 3808.526855469\n",
      " Train accuracy: 0.62611\n",
      " Test accuracy: 0.212766\n",
      "Epoch: 0022 cost= 3719.130104759\n",
      " Train accuracy: 0.638544\n",
      " Test accuracy: 0.219858\n",
      "Epoch: 0023 cost= 3439.403231534\n",
      " Train accuracy: 0.661634\n",
      " Test accuracy: 0.219858\n",
      "Epoch: 0024 cost= 3213.875355114\n",
      " Train accuracy: 0.688277\n",
      " Test accuracy: 0.219858\n",
      "Epoch: 0025 cost= 3193.619273793\n",
      " Train accuracy: 0.708703\n",
      " Test accuracy: 0.223404\n",
      "Epoch: 0026 cost= 2846.285777699\n",
      " Train accuracy: 0.707815\n",
      " Test accuracy: 0.219858\n",
      "Epoch: 0027 cost= 3055.807439631\n",
      " Train accuracy: 0.728242\n",
      " Test accuracy: 0.205674\n",
      "Epoch: 0028 cost= 2628.514559659\n",
      " Train accuracy: 0.741563\n",
      " Test accuracy: 0.205674\n",
      "Epoch: 0029 cost= 2579.309148615\n",
      " Train accuracy: 0.746004\n",
      " Test accuracy: 0.219858\n",
      "Epoch: 0030 cost= 2510.122048118\n",
      " Train accuracy: 0.756661\n",
      " Test accuracy: 0.234043\n",
      "Epoch: 0031 cost= 2399.558172053\n",
      " Train accuracy: 0.774423\n",
      " Test accuracy: 0.22695\n",
      "Epoch: 0032 cost= 2375.203258168\n",
      " Train accuracy: 0.782416\n",
      " Test accuracy: 0.223404\n",
      "Epoch: 0033 cost= 2298.355601918\n",
      " Train accuracy: 0.79929\n",
      " Test accuracy: 0.212766\n",
      "Epoch: 0034 cost= 2299.660311612\n",
      " Train accuracy: 0.805506\n",
      " Test accuracy: 0.219858\n",
      "Epoch: 0035 cost= 2121.903375799\n",
      " Train accuracy: 0.80373\n",
      " Test accuracy: 0.20922\n",
      "Epoch: 0036 cost= 1943.966441761\n",
      " Train accuracy: 0.821492\n",
      " Test accuracy: 0.219858\n",
      "Epoch: 0037 cost= 1909.107743697\n",
      " Train accuracy: 0.833037\n",
      " Test accuracy: 0.216312\n",
      "Epoch: 0038 cost= 1832.203801935\n",
      " Train accuracy: 0.838366\n",
      " Test accuracy: 0.212766\n",
      "Epoch: 0039 cost= 1803.619506836\n",
      " Train accuracy: 0.834813\n",
      " Test accuracy: 0.219858\n",
      "Epoch: 0040 cost= 1740.554620916\n",
      " Train accuracy: 0.845471\n",
      " Test accuracy: 0.216312\n",
      "Epoch: 0041 cost= 1787.471313477\n",
      " Train accuracy: 0.854352\n",
      " Test accuracy: 0.212766\n",
      "Epoch: 0042 cost= 1721.317560369\n",
      " Train accuracy: 0.868561\n",
      " Test accuracy: 0.212766\n",
      "Epoch: 0043 cost= 1541.126220703\n",
      " Train accuracy: 0.868561\n",
      " Test accuracy: 0.234043\n",
      "Epoch: 0044 cost= 1443.598066850\n",
      " Train accuracy: 0.875666\n",
      " Test accuracy: 0.22695\n",
      "Epoch: 0045 cost= 1445.221624201\n",
      " Train accuracy: 0.880107\n",
      " Test accuracy: 0.230496\n",
      "Epoch: 0046 cost= 1414.640480735\n",
      " Train accuracy: 0.889876\n",
      " Test accuracy: 0.216312\n",
      "Epoch: 0047 cost= 1323.790238814\n",
      " Train accuracy: 0.895204\n",
      " Test accuracy: 0.219858\n",
      "Epoch: 0048 cost= 1281.861716531\n",
      " Train accuracy: 0.903197\n",
      " Test accuracy: 0.216312\n",
      "Epoch: 0049 cost= 1225.410139604\n",
      " Train accuracy: 0.901421\n",
      " Test accuracy: 0.205674\n",
      "Epoch: 0050 cost= 1208.086137251\n",
      " Train accuracy: 0.903197\n",
      " Test accuracy: 0.205674\n",
      "Optimization Finished!\n",
      "Accuracy: 0.205674\n"
     ]
    }
   ],
   "source": [
    "def get_batch(X, Y, current_batch, batch_size):\n",
    "    batch_x = X[current_batch*batch_size : (current_batch+1)*batch_size, :]\n",
    "    batch_y = Y[current_batch*batch_size : (current_batch+1)*batch_size, :]\n",
    "    #print(current_batch*batch_size, (current_batch+1) * batch_size)\n",
    "    return batch_x, batch_y\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 50\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 512 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 4096 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 5 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases, keep_prob):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    " \n",
    "\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(train_features.shape[0]/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = get_batch(train_features, train_labels, i, batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y,\n",
    "                                                          keep_prob: 0.2})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "            print(\" Train accuracy:\", accuracy.eval(feed_dict = {x: train_features, y: train_labels, keep_prob : 1.0}))\n",
    "            \n",
    "            print(\" Test accuracy:\", accuracy.eval(feed_dict = {x: test_features, y: test_labels, keep_prob : 1.0}))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    print(\"Accuracy:\", accuracy.eval(feed_dict ={x: test_features, y: test_labels, keep_prob : 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'album_id': '72157623139776636',\n",
       " 'id': '223009',\n",
       " 'img_id': '4250204464',\n",
       " 'length': 8,\n",
       " 'order': 4,\n",
       " 'original_text': 'I myself prefer to use my feet.',\n",
       " 'setting': 'first-2-pick-and-tell',\n",
       " 'story_id': '44601',\n",
       " 'text': 'i myself prefer to use my feet .',\n",
       " 'tier': 'story-in-sequence',\n",
       " 'worker_id': 'AS2MDFCRUS236TU'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sis.Sents['223009']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
